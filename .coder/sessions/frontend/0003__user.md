A requisição para a rota /send-message está retornando 422, segue o código da rota, adapte o envio da mensagem:

from fastapi import APIRouter, HTTPException
from typing import List
import httpx
import os
from dotenv import load_dotenv, find_dotenv
from ..models.prompt import PromptResponse
import logging
import json


# Load environment variables from .env file
load_dotenv(find_dotenv())

router = APIRouter()

EXTERNAL_API_TOKEN = os.getenv("EXTERNAL_API_TOKEN")
LLM_API_URL = os.getenv("LLM_API_URL")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME")
LLM_PROVIDER = os.getenv("LLM_PROVIDER")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", 0.5))
DEFAULT_PROMPT_TITLE = os.getenv("DEFAULT_PROMPT_TITLE")
DEFAULT_PROMPT_DESCRIPTION = os.getenv("DEFAULT_PROMPT_DESCRIPTION")
DEFAULT_PROMPT_ID = os.getenv("DEFAULT_PROMPT_ID")
FLOW_MODEL = os.getenv("FLOW_MODEL")
FLOW_CHANNEL = os.getenv("FLOW_CHANNEL")
FLOW_AGENT = os.getenv("FLOW_AGENT")

# Log the loaded environment variables for debugging
logging.info(f"EXTERNAL_API_TOKEN: {EXTERNAL_API_TOKEN}")
logging.info(f"LLM_API_URL: {LLM_API_URL}")

if not LLM_API_URL:
    raise ValueError("LLM_API_URL environment variable is not set")

@router.post("/send-message", response_model=PromptResponse, tags=["channels"])
async def send_message_to_llm(prompt: str):
    url = LLM_API_URL
    headers = {
        "Content-Type": "application/json",
        "flowModel": FLOW_MODEL,
        "flowChannel": FLOW_CHANNEL,
        "flowAgent": FLOW_AGENT,
        "Authorization": f"Bearer {EXTERNAL_API_TOKEN}"  # Use token from environment variable
    }
    payload = {
        "content": [
            {
                "type": "text/plain",
                "value": prompt
            }
        ],
        "model": {
            "name": LLM_MODEL_NAME,
            "provider": LLM_PROVIDER,
            "modelSettings": [
                {
                    "name": "temperature",
                    "value": LLM_TEMPERATURE
                }
            ]
        },
        "agent": FLOW_CHANNEL,
        "sources": [],
        "connectors": [],
        "operation": "new-question",
    }

    async with httpx.AsyncClient(timeout=None) as client:
        try:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            # Process the streaming response
            response_text = ""
            async for chunk in response.aiter_text():
                response_text += chunk
        except httpx.HTTPStatusError as exc:
            logging.error(f"Error response {exc.response.status_code} while requesting {exc.request.url!r}.")
            logging.error(f"Response content: {exc.response.text}")
            raise HTTPException(status_code=exc.response.status_code, detail=f"Error from external API: {exc.response.text}")
        
        # Extract the content from the response text
        content = extract_content_from_response(response_text)
        
        # Return the concatenated response as JSON
        return PromptResponse(title=DEFAULT_PROMPT_TITLE, description=DEFAULT_PROMPT_DESCRIPTION, prompt=content, id=DEFAULT_PROMPT_ID)

def extract_content_from_response(response_text: str) -> str:
    content = ""
    for line in response_text.split("\n"):
        if line.strip():
            try:
                data = json.loads(line)
                if "content" in data:
                    for item in data["content"]:
                        if item["type"] == "text/plain":
                            content += item["value"]
            except json.JSONDecodeError:
                continue
    return content